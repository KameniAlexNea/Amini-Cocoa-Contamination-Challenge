{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55191e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09986a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"dataset/validations/predictions.csv\")\n",
    "predictions = predictions.rename(\n",
    "    columns={\"x_min\": \"xmin\", \"y_min\": \"ymin\", \"x_max\": \"xmax\", \"y_max\": \"ymax\"}\n",
    ")\n",
    "predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ce7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels = predictions[\"Image_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e17286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df: pd.DataFrame):\n",
    "\tdf = df.copy().dropna()\n",
    "\treturn {\n",
    "\t\timg_id: {\n",
    "\t\t\t\"boxes\": torch.tensor(raw[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, dtype=torch.float32),\n",
    "\t\t\t\"scores\": (\n",
    "\t\t\t\ttorch.tensor(raw[\"confidence\"].values, dtype=torch.float32)\n",
    "\t\t\t\tif \"confidence\" in raw.columns\n",
    "\t\t\t\telse None\n",
    "\t\t\t),\n",
    "\t\t\t\"labels\": torch.tensor(raw[\"class_id\"].values, dtype=torch.int32),\n",
    "\t\t}\n",
    "\t\tfor (img_id, ), raw in df.groupby([\"Image_ID\"])\n",
    "\t}\n",
    "\n",
    "def default_value():\n",
    "\treturn {\n",
    "\t\t\"boxes\": torch.empty((0, 4), dtype=torch.float32),\n",
    "\t\t\"scores\": torch.empty((0,), dtype=torch.float32),\n",
    "\t\t\"labels\": torch.empty((0,), dtype=torch.int32),\n",
    "\t}\n",
    "\n",
    "def get_preds_data(preds, thr: float = 0.5):\n",
    "\tif thr is not None:\n",
    "\t\tpreds = preds[preds[\"confidence\"] >= thr]\n",
    "\tpreds = convert_df(preds)\n",
    "\td = default_value()\n",
    "\treturn {i: preds.get(i, d) for i in converted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_preds_data(predictions, thr=None)\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "validations = pd.read_csv(\"dataset/Val_df.csv\")\n",
    "validations.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = get_preds_data(validations, thr=None)\n",
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa51813",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals[\"ID_B9K2SI.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou_tensor(box1, box2):\n",
    "\t\"\"\"\n",
    "\tbox1: [4], box2: [4]\n",
    "\tFormat: [xmin, ymin, xmax, ymax]\n",
    "\t\"\"\"\n",
    "\txA = torch.max(box1[0], box2[0])\n",
    "\tyA = torch.max(box1[1], box2[1])\n",
    "\txB = torch.min(box1[2], box2[2])\n",
    "\tyB = torch.min(box1[3], box2[3])\n",
    "\n",
    "\tinter_area = torch.clamp(xB - xA, min=0) * torch.clamp(yB - yA, min=0)\n",
    "\tbox1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "\tbox2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\tunion_area = box1_area + box2_area - inter_area\n",
    "\treturn inter_area / union_area if union_area > 0 else torch.tensor(0.0)\n",
    "\n",
    "def evaluate_detection(predictions, ground_truths, iou_threshold=0.5, conf_threshold=0.0):\n",
    "\t\"\"\"\n",
    "\tpredictions: list of dicts (len = batch size), each dict with 'boxes', 'scores', 'labels'\n",
    "\tground_truths: list of dicts with 'boxes', 'labels'\n",
    "\t\"\"\"\n",
    "\tTP = 0\n",
    "\tFP = 0\n",
    "\tFN = 0\n",
    "\n",
    "\tfor preds, gts in zip(predictions, ground_truths):\n",
    "\t\tpred_boxes = preds['boxes']\n",
    "\t\tpred_labels = preds['labels']\n",
    "\t\tpred_scores = preds['scores'] if preds['scores'] is not None else torch.ones(len(pred_boxes))\n",
    "\n",
    "\t\tgt_boxes = gts['boxes']\n",
    "\t\tgt_labels = gts['labels']\n",
    "\t\tmatched_gt = set()\n",
    "\n",
    "\t\tfor i in range(len(pred_boxes)):\n",
    "\t\t\tif pred_scores[i] < conf_threshold:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpred_box = pred_boxes[i]\n",
    "\t\t\tpred_label = pred_labels[i]\n",
    "\t\t\tmatch_found = False\n",
    "\n",
    "\t\t\tfor j in range(len(gt_boxes)):\n",
    "\t\t\t\tif j in matched_gt:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif pred_label != gt_labels[j]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tiou = calculate_iou_tensor(pred_box, gt_boxes[j])\n",
    "\t\t\t\tif iou >= iou_threshold:\n",
    "\t\t\t\t\tTP += 1\n",
    "\t\t\t\t\tmatched_gt.add(j)\n",
    "\t\t\t\t\tmatch_found = True\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif not match_found:\n",
    "\t\t\t\tFP += 1\n",
    "\n",
    "\t\tFN += len(gt_boxes) - len(matched_gt)\n",
    "\n",
    "\tprecision = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "\trecall = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "\tf1_score = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\taccuracy = TP / (TP + FP + FN) if (TP + FP + FN) else 0.0\n",
    "\n",
    "\treturn {\n",
    "\t\t'TP': TP,\n",
    "\t\t'FP': FP,\n",
    "\t\t'FN': FN,\n",
    "\t\t'Precision': precision,\n",
    "\t\t'Recall': recall,\n",
    "\t\t'F1 Score': f1_score,\n",
    "\t\t'Accuracy': accuracy\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.linspace(0.0, 0.95, 15):\n",
    "\tscores = evaluate_detection(\n",
    "\t\tpreds.values(),\n",
    "\t\tvals.values(),\n",
    "\t\tiou_threshold=0.5,\n",
    "\t\tconf_threshold=i\n",
    "\t)\n",
    "\tprint(\"Evaluation metric at:\", i, \" score :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "def compute_map(preds, targets, iou_thresholds):\n",
    "\t\"\"\"\n",
    "\tCompute mAP at different IoU thresholds using torchmetrics.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds: List of dicts with 'boxes', 'scores', 'labels' for predictions\n",
    "\t\ttargets: List of dicts with 'boxes', 'labels' for ground truth\n",
    "\t\tiou_thresholds: List of IoU thresholds to evaluate\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tDict containing mAP results for each IoU threshold\n",
    "\t\"\"\"\n",
    "\t# Initialize the metric\n",
    "\tmetric = MeanAveragePrecision(iou_thresholds=iou_thresholds)\n",
    "\t\n",
    "\t# Update metric with predictions and targets\n",
    "\tmetric.update(preds, targets)\n",
    "\t\n",
    "\t# Compute the results\n",
    "\tresult = metric.compute()\n",
    "\t\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea643c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs = np.linspace(0.0, 0.95, 15)\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "# Example predictions and targets\n",
    "targets = list(vals.values())\n",
    "\n",
    "iou_thresholds = [0.5]\n",
    "for i in thrs:\n",
    "\tpreds = list(get_preds_data(predictions, i).values())\n",
    "\n",
    "\t# Compute mAP\n",
    "\tresults = compute_map(preds, targets, iou_thresholds)\n",
    "\n",
    "\t# Print results\n",
    "\tprint(\"mAP Results:\", i, \" - \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19e5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
