{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from PIL import Image, ExifTags\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIRS\n",
    "INPUT_DATA_DIR = Path('dataset')\n",
    "## Drop the Folder if it already exists\n",
    "DATASETS_DIR = Path('dataset')\n",
    "# Image & labels directory\n",
    "TRAIN_IMAGES_DIR = DATASETS_DIR / 'images' / 'train'\n",
    "TRAIN_LABELS_DIR = DATASETS_DIR / 'labels'/ 'train'\n",
    "TEST_IMAGES_DIR = DATASETS_DIR / 'images' / 'test'\n",
    "VAL_IMAGES_DIR = DATASETS_DIR / 'images' /'val'\n",
    "VAL_LABELS_DIR = DATASETS_DIR / 'labels' /'val'\n",
    "\n",
    "# Load train and test files\n",
    "train = pd.read_csv(INPUT_DATA_DIR / 'Train_df.csv')\n",
    "val = pd.read_csv(INPUT_DATA_DIR / 'Val_df.csv')\n",
    "test = pd.read_csv(INPUT_DATA_DIR / 'Test.csv')\n",
    "ss = pd.read_csv(INPUT_DATA_DIR / 'SampleSubmission.csv')\n",
    "\n",
    "class_map = {cls: i for i, cls in enumerate(sorted(train['class'].unique().tolist()))}\n",
    "# Strip any spacing from the class item and make sure that it is a str\n",
    "train['class'] = train['class'].str.strip()\n",
    "\n",
    "# Map {'healthy': 2, 'cssvd': 1, anthracnose: 0}\n",
    "train['class_id'] = train['class'].map(class_map)\n",
    "\n",
    "train_df = train\n",
    "val_df = val\n",
    "\n",
    "# Create a data.yaml file required by yolo\n",
    "class_names = sorted(train['class'].unique().tolist())\n",
    "num_classes = len(class_names)\n",
    "data_yaml = {\n",
    "\t\"path\" : str(DATASETS_DIR.absolute()),\n",
    "\t'train': str(TRAIN_IMAGES_DIR.absolute()),\n",
    "\t'val': str(VAL_IMAGES_DIR.absolute()),\n",
    "\t'test': str(TEST_IMAGES_DIR.absolute()),\n",
    "\t'nc': num_classes,\n",
    "\t'names': class_names\n",
    "}\n",
    "\n",
    "val_image_names = [str(Path(name).stem) for name in val_df['Image_ID'].unique()]\n",
    "train_image_names = [str(Path(name).stem) for name in train['ImagePath'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['Image_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Validate the model on the validation set\n",
    "BEST_PATH = sorted(glob(\"runs/detect/train*/weights/best.pt\"))[-1]\n",
    "BEST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained YOLO model\n",
    "model = YOLO(BEST_PATH)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flag, v in ExifTags.TAGS.items():\n",
    "    if v == \"Orientation\":\n",
    "        break\n",
    "\n",
    "\n",
    "def load_image(filepath):\n",
    "    image = Image.open(filepath)\n",
    "\n",
    "    exif = image._getexif()\n",
    "    if exif is None:\n",
    "        return image\n",
    "        # return torch.tensor(np.array(image.convert(\"RGB\")))\n",
    "\n",
    "    orientation_value = exif.get(flag, None)\n",
    "\n",
    "    if orientation_value == 3:\n",
    "        image = image.rotate(180, expand=True)\n",
    "    elif orientation_value == 6:\n",
    "        image = image.rotate(270, expand=True)\n",
    "    elif orientation_value == 8:\n",
    "        image = image.rotate(90, expand=True)\n",
    "    return image\n",
    "    # return torch.tensor(np.array(image.convert(\"RGB\")))\n",
    "\n",
    "\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model on the validation set\n",
    "BEST_CFG = sorted(glob(\"runs/detect/train*/args.yaml\"))[-1]\n",
    "BEST_CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the test images directory\n",
    "test_dir_path = VAL_IMAGES_DIR\n",
    "\n",
    "# Get a list of all image files in the test directory\n",
    "image_files = [i for i in os.listdir(test_dir_path) if not i.endswith(\".npy\")]\n",
    "\n",
    "# Initialize an empty list to store the results for all images\n",
    "all_data = []\n",
    "\n",
    "# Initialize an empty list to store the results for all images\n",
    "all_data = []\n",
    "\n",
    "# Batch size for predictions\n",
    "batch_size = 16\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(image_files), batch_size)):\n",
    "\tbatch_files = image_files[i:i + batch_size]\n",
    "\tbatch_images = [load_image(os.path.join(test_dir_path, img_file)) for img_file in batch_files]\n",
    "\n",
    "\t# Make predictions on the batch of images\n",
    "\tresults = model.predict(\n",
    "\t\tbatch_images,\n",
    "\t\tconf=0.,\n",
    "\t\timgsz=1024,\n",
    "\t\tbatch=batch_size,\n",
    "\t\tverbose=False,\n",
    "\t\tdevice=\"cuda:0\",\n",
    "\n",
    "\t\tseed=0,\n",
    "\t\tdeterministic=True,\n",
    "\t\tclose_mosaic=10,\n",
    "\t\tmask_ratio=4,\n",
    "\t\tiou=.7,\n",
    "\t\tmax_det=100,\n",
    "\t\tformat=\"torchscript\",\n",
    "\t\tnms=True,\n",
    "\t\tsimplify=True,\n",
    "\t\tvid_stride=1,\n",
    "\t\tcfg=BEST_CFG,\n",
    "\t)\n",
    "\n",
    "\t# Iterate through each result in the batch\n",
    "\tfor img_file, result in zip(batch_files, results):\n",
    "\t\tboxes = result.boxes.xyxy.tolist() if result.boxes else []  # Bounding boxes in xyxy format\n",
    "\t\tclasses = result.boxes.cls.tolist() if result.boxes else []  # Class indices\n",
    "\t\tconfidences = result.boxes.conf.tolist() if result.boxes else []  # Confidence scores\n",
    "\t\tnames = result.names  # Class names dictionary\n",
    "\n",
    "\t\tif boxes:  # If detections are found\n",
    "\t\t\tfor box, cls, conf in zip(boxes, classes, confidences):\n",
    "\t\t\t\tx1, y1, x2, y2 = box\n",
    "\t\t\t\tdetected_class = names[int(cls)]  # Get the class name from the names dictionary\n",
    "\n",
    "\t\t\t\t# Add the result to the all_data list\n",
    "\t\t\t\tall_data.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"Image_ID\": str(img_file),\n",
    "\t\t\t\t\t\t\"class\": detected_class,\n",
    "\t\t\t\t\t\t\"confidence\": conf,\n",
    "\t\t\t\t\t\t\"ymin\": y1,\n",
    "\t\t\t\t\t\t\"xmin\": x1,\n",
    "\t\t\t\t\t\t\"ymax\": y2,\n",
    "\t\t\t\t\t\t\"xmax\": x2,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\telse:  # If no objects are detected\n",
    "\t\t\tall_data.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"Image_ID\": str(img_file),\n",
    "\t\t\t\t\t\"class\": \"None\",\n",
    "\t\t\t\t\t\"confidence\": None,\n",
    "\t\t\t\t\t\"ymin\": None,\n",
    "\t\t\t\t\t\"xmin\": None,\n",
    "\t\t\t\t\t\"ymax\": None,\n",
    "\t\t\t\t\t\"xmax\": None,\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a DataFrame for all images\n",
    "sub = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_labels(label_folder):\n",
    "\tlabel_data = {}\n",
    "\tlabel_folder = Path(label_folder)\n",
    "\tpaths = [i for i in label_folder.glob(\"*\") if i.suffix != \".npy\"]\n",
    "\n",
    "\tfor label_file in paths:\n",
    "\t\twith open(label_file, \"r\") as file:\n",
    "\t\t\tannotations = []\n",
    "\t\t\tfor line in file:\n",
    "\t\t\t\tparts = line.strip().split()\n",
    "\t\t\t\tif len(parts) == 5:\n",
    "\t\t\t\t\tclass_id, x_center, y_center, width, height = map(float, parts)\n",
    "\t\t\t\t\tannotations.append({\n",
    "\t\t\t\t\t\t\"class_id\": int(class_id),\n",
    "\t\t\t\t\t\t\"x_center\": x_center,\n",
    "\t\t\t\t\t\t\"y_center\": y_center,\n",
    "\t\t\t\t\t\t\"width\": width,\n",
    "\t\t\t\t\t\t\"height\": height\n",
    "\t\t\t\t\t})\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"Skipping line in {label_file}: {line.strip()}\")\n",
    "\t\t\tlabel_data[label_file.stem] = annotations\n",
    "\t# Convert the label data to a pandas DataFrame\n",
    "\tlabel_df = []\n",
    "\tfor image_id, annotations in label_data.items():\n",
    "\t\tfor annotation in annotations:\n",
    "\t\t\tlabel_df.append({\n",
    "\t\t\t\t\"Image_ID\": image_id,\n",
    "\t\t\t\t\"class_id\": annotation[\"class_id\"],\n",
    "\t\t\t\t\"x_center\": annotation[\"x_center\"],\n",
    "\t\t\t\t\"y_center\": annotation[\"y_center\"],\n",
    "\t\t\t\t\"width\": annotation[\"width\"],\n",
    "\t\t\t\t\"height\": annotation[\"height\"]\n",
    "\t\t\t})\n",
    "\n",
    "\tlabel_df = pd.DataFrame(label_df)\n",
    "\treturn label_df\n",
    "\n",
    "# Example usage\n",
    "label_folder = VAL_LABELS_DIR\n",
    "labels = load_yolo_labels(label_folder)\n",
    "labels.sample(5)\n",
    "\n",
    "def yolo_to_bbox(image_folder, labels_df: pd.DataFrame):\n",
    "\timage_folder = Path(image_folder)\n",
    "\tconverted_bboxes = []\n",
    "\n",
    "\tpaths = [i for i in image_folder.glob(\"*\") if i.suffix != \".npy\"]\n",
    "\tfor image_file in paths:\n",
    "\t\timage_id = image_file.stem\n",
    "\t\tif image_id not in labels_df['Image_ID'].values:\n",
    "\t\t\tconverted_bboxes.append({\n",
    "\t\t\t\t\"Image_ID\": image_id,\n",
    "\t\t\t\t\"class_id\": -1,  # Indicating no label\n",
    "\t\t\t\t\"xmin\": None,\n",
    "\t\t\t\t\"ymin\": None,\n",
    "\t\t\t\t\"xmax\": None,\n",
    "\t\t\t\t\"ymax\": None\n",
    "\t\t\t})\n",
    "\n",
    "\tfor _, row in labels_df.iterrows():\n",
    "\t\tall_ids = [i for i in image_folder.glob(f\"{row['Image_ID']}*\") if i.suffix != \".npy\"]\n",
    "\t\timage_path = image_folder / f\"{row['Image_ID']}\"\n",
    "\t\tif all_ids:\n",
    "\t\t\timage_path = all_ids[0]\n",
    "\n",
    "\t\tif image_path.exists():\n",
    "\t\t\timg = load_image(image_path)\n",
    "\t\t\timg_width, img_height = img.size\n",
    "\n",
    "\t\t\tx_center = row['x_center'] * img_width\n",
    "\t\t\ty_center = row['y_center'] * img_height\n",
    "\t\t\twidth = row['width'] * img_width\n",
    "\t\t\theight = row['height'] * img_height\n",
    "\n",
    "\t\t\tx_min = x_center - (width / 2)\n",
    "\t\t\ty_min = y_center - (height / 2)\n",
    "\t\t\tx_max = x_center + (width / 2)\n",
    "\t\t\ty_max = y_center + (height / 2)\n",
    "\n",
    "\t\t\tconverted_bboxes.append({\n",
    "\t\t\t\t\"Image_ID\": row['Image_ID'],\n",
    "\t\t\t\t\"class_id\": row['class_id'],\n",
    "\t\t\t\t\"xmin\": x_min,\n",
    "\t\t\t\t\"ymin\": y_min,\n",
    "\t\t\t\t\"xmax\": x_max,\n",
    "\t\t\t\t\"ymax\": y_max\n",
    "\t\t\t})\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Image {image_path} not found.\")\n",
    "\n",
    "\treturn pd.DataFrame(converted_bboxes)\n",
    "\n",
    "# Example usage\n",
    "converted_labels = yolo_to_bbox(VAL_IMAGES_DIR, labels)\n",
    "converted_labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels[\"Image_ID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels['class_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_class_map = {v: k for k, v in class_map.items()}\n",
    "converted_labels['class'] = converted_labels['class_id'].map(id_class_map)\n",
    "converted_labels['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[:, \"Image_ID\"] = sub[\"Image_ID\"].apply(lambda x: str(Path(x).stem))\n",
    "\n",
    "sub.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df: pd.DataFrame):\n",
    "\tdf = df.copy().dropna()\n",
    "\treturn {\n",
    "\t\timg_id: {\n",
    "\t\t\t\"boxes\": torch.tensor(raw[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values, dtype=torch.float32),\n",
    "\t\t\t\"scores\": (\n",
    "\t\t\t\ttorch.tensor(raw[\"confidence\"].values, dtype=torch.float32)\n",
    "\t\t\t\tif \"confidence\" in raw.columns\n",
    "\t\t\t\telse None\n",
    "\t\t\t),\n",
    "\t\t\t\"labels\": torch.tensor(raw[\"class_id\"].values, dtype=torch.int32),\n",
    "\t\t}\n",
    "\t\tfor (img_id, ), raw in df.groupby([\"Image_ID\"])\n",
    "\t}\n",
    "\n",
    "def default_value():\n",
    "\treturn {\n",
    "\t\t\"boxes\": torch.empty((0, 4), dtype=torch.float32),\n",
    "\t\t\"scores\": torch.empty((0,), dtype=torch.float32),\n",
    "\t\t\"labels\": torch.empty((0,), dtype=torch.int32),\n",
    "\t}\n",
    "\n",
    "def get_preds_data(preds, thr: float = 0.5):\n",
    "\tif thr is not None:\n",
    "\t\tpreds = preds[preds[\"confidence\"] >= thr]\n",
    "\tpreds = convert_df(preds)\n",
    "\td = default_value()\n",
    "\treturn {i: preds.get(i, d) for i in converted_labels[\"Image_ID\"].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = convert_df(converted_labels)\n",
    "ground_truth = {k: ground_truth[k] for k in converted_labels[\"Image_ID\"].unique()}\n",
    "\n",
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou_tensor(box1, box2):\n",
    "\t\"\"\"\n",
    "\tbox1: [4], box2: [4]\n",
    "\tFormat: [xmin, ymin, xmax, ymax]\n",
    "\t\"\"\"\n",
    "\txA = torch.max(box1[0], box2[0])\n",
    "\tyA = torch.max(box1[1], box2[1])\n",
    "\txB = torch.min(box1[2], box2[2])\n",
    "\tyB = torch.min(box1[3], box2[3])\n",
    "\n",
    "\tinter_area = torch.clamp(xB - xA, min=0) * torch.clamp(yB - yA, min=0)\n",
    "\tbox1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "\tbox2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\tunion_area = box1_area + box2_area - inter_area\n",
    "\treturn inter_area / union_area if union_area > 0 else torch.tensor(0.0)\n",
    "\n",
    "def evaluate_detection(predictions, ground_truths, iou_threshold=0.5, conf_threshold=0.0):\n",
    "\t\"\"\"\n",
    "\tpredictions: list of dicts (len = batch size), each dict with 'boxes', 'scores', 'labels'\n",
    "\tground_truths: list of dicts with 'boxes', 'labels'\n",
    "\t\"\"\"\n",
    "\tTP = 0\n",
    "\tFP = 0\n",
    "\tFN = 0\n",
    "\n",
    "\tfor preds, gts in zip(predictions, ground_truths):\n",
    "\t\tpred_boxes = preds['boxes']\n",
    "\t\tpred_labels = preds['labels']\n",
    "\t\tpred_scores = preds['scores'] if preds['scores'] is not None else torch.ones(len(pred_boxes))\n",
    "\n",
    "\t\tgt_boxes = gts['boxes']\n",
    "\t\tgt_labels = gts['labels']\n",
    "\t\tmatched_gt = set()\n",
    "\n",
    "\t\tfor i in range(len(pred_boxes)):\n",
    "\t\t\tif pred_scores[i] < conf_threshold:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpred_box = pred_boxes[i]\n",
    "\t\t\tpred_label = pred_labels[i]\n",
    "\t\t\tmatch_found = False\n",
    "\n",
    "\t\t\tfor j in range(len(gt_boxes)):\n",
    "\t\t\t\tif j in matched_gt:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif pred_label != gt_labels[j]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tiou = calculate_iou_tensor(pred_box, gt_boxes[j])\n",
    "\t\t\t\tif iou >= iou_threshold:\n",
    "\t\t\t\t\tTP += 1\n",
    "\t\t\t\t\tmatched_gt.add(j)\n",
    "\t\t\t\t\tmatch_found = True\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif not match_found:\n",
    "\t\t\t\tFP += 1\n",
    "\n",
    "\t\tFN += len(gt_boxes) - len(matched_gt)\n",
    "\n",
    "\tprecision = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "\trecall = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "\tf1_score = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\taccuracy = TP / (TP + FP + FN) if (TP + FP + FN) else 0.0\n",
    "\n",
    "\treturn {\n",
    "\t\t'TP': TP,\n",
    "\t\t'FP': FP,\n",
    "\t\t'FN': FN,\n",
    "\t\t'Precision': precision,\n",
    "\t\t'Recall': recall,\n",
    "\t\t'F1 Score': f1_score,\n",
    "\t\t'Accuracy': accuracy\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[\"class_id\"] = sub[\"class\"].map(class_map)\n",
    "\n",
    "sub.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_preds_data(sub, None)\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(0.0, 0.95, 15):\n",
    "\tscores = evaluate_detection(\n",
    "\t\tpredictions.values(),\n",
    "\t\tground_truth.values(),\n",
    "\t\tiou_threshold=0.5,\n",
    "\t\tconf_threshold=i\n",
    "\t)\n",
    "\tprint(\"Evaluation metric at:\", i, \" score :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "def compute_map(preds, targets):\n",
    "\t\"\"\"\n",
    "\tCompute mAP at different IoU thresholds using torchmetrics.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds: List of dicts with 'boxes', 'scores', 'labels' for predictions\n",
    "\t\ttargets: List of dicts with 'boxes', 'labels' for ground truth\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tDict containing mAP results for each IoU threshold\n",
    "\t\"\"\"\n",
    "\t# Initialize the metric\n",
    "\tmetric = MeanAveragePrecision()\n",
    "\t\n",
    "\t# Update metric with predictions and targets\n",
    "\tmetric.update(preds, targets)\n",
    "\t\n",
    "\t# Compute the results\n",
    "\tresult = metric.compute()\n",
    "\t\n",
    "\treturn result\n",
    "\n",
    "thrs = np.linspace(0.0001, 0.95, 15)\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "# Example predictions and targets\n",
    "# iou_thresholds = [0.5]\n",
    "for i in thrs:\n",
    "\tpreds = list(get_preds_data(sub, i).values())\n",
    "\n",
    "\ttargets = list(ground_truth.values())\n",
    "\n",
    "\t# Compute mAP\n",
    "\tresults = compute_map(preds, targets)\n",
    "\n",
    "\t# Print results\n",
    "\tprint(\"mAP Results:\", i, \" - \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('dataset/evaluations/validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sub = pd.read_csv('dataset/evaluations/validation.csv')\n",
    "sub.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
